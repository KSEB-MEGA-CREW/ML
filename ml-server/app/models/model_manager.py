# app/models/model_manager.py
import asyncio
import logging
import numpy as np
from typing import Dict, Any, Optional
import concurrent.futures
import os
import tensorflow as tf
from pathlib import Path
import time

logger = logging.getLogger(__name__)


class ModelManager:
    def __init__(self):
        self.model = None
        self.is_loaded = False
        self.executor = None
        self.model_path = None
        self.class_labels = []
        self._initialization_lock = asyncio.Lock()

    async def initialize(self, model_path: str = None):
        """ë¹„ë™ê¸° ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ë³´ì¥"""
        async with self._initialization_lock:
            if not self.is_loaded:
                success = await self.load_model(model_path)
                return success
            return True

    async def load_model(self, model_path: str = None):
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ"""
        try:
            # ëª¨ë¸ ê²½ë¡œ í™•ì¸
            self.model_path = self._find_model_path(model_path)
            if not self.model_path:
                raise FileNotFoundError("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {self.model_path}")

            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ë¡œë“œ (I/O ë¸”ë¡œí‚¹ ë°©ì§€)
            loop = asyncio.get_event_loop()
            model_data = await loop.run_in_executor(
                None, self._load_model_from_file, self.model_path
            )

            self.model = model_data["model"]
            self.class_labels = model_data["labels"]

            # ì˜ˆì¸¡ìš© ìŠ¤ë ˆë“œ í’€ ìƒì„±
            self.executor = concurrent.futures.ThreadPoolExecutor(
                max_workers=2, thread_name_prefix="model_prediction"
            )

            # ëª¨ë¸ ì›Œë°ì—…
            await self._warmup_model()

            self.is_loaded = True
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.class_labels)}ê°œ í´ë˜ìŠ¤")

            return True

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            return False

    def _find_model_path(self, model_path: str = None) -> Optional[str]:
        """ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        if model_path and os.path.exists(model_path):
            return model_path

        # ê¸°ë³¸ ê²½ë¡œë“¤ ê²€ìƒ‰
        possible_paths = [
            "./models/sign_language_model.h5",
            "./models/sign_language_model.keras",
            "./models/sign_language_model",
            "../models/sign_language_model.h5",
            "../../models/sign_language_model.h5",
            os.getenv("MODEL_PATH", "./models/sign_language_model.h5"),
        ]

        for path in possible_paths:
            if os.path.exists(path):
                return path

        return None

    def _load_model_from_file(self, model_path: str) -> Dict[str, Any]:
        """íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ (ë™ê¸° - ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        try:
            # TensorFlow ëª¨ë¸ ë¡œë“œ
            if model_path.endswith((".h5", ".keras")):
                model = tf.keras.models.load_model(model_path, compile=False)
            else:
                model = tf.saved_model.load(model_path)

            # í´ë˜ìŠ¤ ë ˆì´ë¸” ë¡œë“œ
            labels = self._load_class_labels(model_path)

            logger.info(
                f"ğŸ—ï¸ ëª¨ë¸ ì •ë³´: input_shape={getattr(model, 'input_shape', 'Unknown')}"
            )

            return {"model": model, "labels": labels}

        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def _load_class_labels(self, model_path: str) -> list:
        """í´ë˜ìŠ¤ ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ"""
        try:
            base_path = Path(model_path).parent
            label_files = [
                base_path / "class_labels.txt",
                base_path / "labels.txt",
                base_path / "classes.txt",
            ]

            for label_file in label_files:
                if label_file.exists():
                    with open(label_file, "r", encoding="utf-8") as f:
                        labels = [
                            line.strip() for line in f.readlines() if line.strip()
                        ]
                    logger.info(f"ğŸ“‹ ë ˆì´ë¸” íŒŒì¼ ë¡œë“œ: {len(labels)}ê°œ")
                    return labels

            # ê¸°ë³¸ ìˆ˜ì–´ ë ˆì´ë¸”
            logger.warning("âš ï¸ ë ˆì´ë¸” íŒŒì¼ ì—†ìŒ - ê¸°ë³¸ ë ˆì´ë¸” ì‚¬ìš©")
            return [
                "ì¢‹ë‹¤1",
                "ì§€ì‹œ1#",
                "ë•ë‹¤1",
                "ë¬´ì—‡1",
                "ì§€ì‹œ2",
                "ë•Œ2",
                "ì˜¤ëŠ˜1",
                "ì¼í•˜ë‹¤1",
                "ì¬ë¯¸1",
                "í•„ìš”1",
                "íšŒì‚¬1",
                "ìš”ë¦¬1",
                "ê´œì°®ë‹¤1",
                "ì˜í•˜ë‹¤2",
                "ê¸°íƒ€",
            ]

        except Exception as e:
            logger.error(f"âŒ ë ˆì´ë¸” ë¡œë“œ ì‹¤íŒ¨: {e}")
            return [f"class_{i}" for i in range(15)]

    async def _warmup_model(self):
        """ëª¨ë¸ ì›Œë°ì—… - ì²« ì˜ˆì¸¡ ì†ë„ í–¥ìƒ"""
        try:
            logger.info("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—…...")
            dummy_input = np.zeros((1, 10, 194), dtype=np.float32)
            result = await self.predict(dummy_input)

            if "error" not in result:
                logger.info("âœ… ì›Œë°ì—… ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ ì›Œë°ì—… ê²½ê³ : {result['error']}")

        except Exception as e:
            logger.warning(f"âš ï¸ ì›Œë°ì—… ì‹¤íŒ¨: {e}")

    async def predict(self, keypoints_batch: np.ndarray) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì˜ˆì¸¡ - ë©”ì¸ ì¸í„°í˜ì´ìŠ¤"""
        # ëª¨ë¸ ì´ˆê¸°í™” í™•ì¸
        if not self.is_loaded:
            init_success = await self.initialize()
            if not init_success:
                return {
                    "label": "ì´ˆê¸°í™”_ì‹¤íŒ¨",
                    "confidence": 0.0,
                    "error": "Model initialization failed",
                }

        try:
            # ì…ë ¥ ê²€ì¦
            validated_input = self._validate_input(keypoints_batch)
            if "error" in validated_input:
                return validated_input

            # ìŠ¤ë ˆë“œ í’€ì—ì„œ ì˜ˆì¸¡ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor, self._predict_in_thread, validated_input["data"]
            )

            return result

        except Exception as e:
            logger.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"label": "ì˜ˆì¸¡_ì‹¤íŒ¨", "confidence": 0.0, "error": str(e)}

    def _validate_input(self, keypoints_batch) -> Dict[str, Any]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        try:
            # NumPy ë°°ì—´ë¡œ ë³€í™˜
            if not isinstance(keypoints_batch, np.ndarray):
                keypoints_batch = np.array(keypoints_batch, dtype=np.float32)

            # í˜•íƒœ ê²€ì¦
            if keypoints_batch.shape != (1, 10, 194):
                return {
                    "error": f"Invalid shape: {keypoints_batch.shape}, expected: (1, 10, 194)"
                }

            # ê°’ ë²”ìœ„ ê²€ì¦ (ì„ íƒì )
            if np.any(np.isnan(keypoints_batch)) or np.any(np.isinf(keypoints_batch)):
                return {"error": "Input contains NaN or Inf values"}

            return {"data": keypoints_batch}

        except Exception as e:
            return {"error": f"Input validation failed: {str(e)}"}

    def _predict_in_thread(self, keypoints_batch: np.ndarray) -> Dict[str, Any]:
        """ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ì‹¤ì œ ì˜ˆì¸¡"""
        try:
            start_time = time.time()

            # ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
            if hasattr(self.model, "predict"):
                # Keras ëª¨ë¸
                predictions = self.model.predict(keypoints_batch, verbose=0)
            else:
                # SavedModel
                predictions = self.model(keypoints_batch).numpy()

            prediction_time = time.time() - start_time

            # ê²°ê³¼ ì²˜ë¦¬
            prediction = predictions[0] if len(predictions.shape) > 1 else predictions
            max_idx = np.argmax(prediction)
            confidence = float(prediction[max_idx])

            # ë ˆì´ë¸” ë§¤í•‘
            label = (
                self.class_labels[max_idx]
                if max_idx < len(self.class_labels)
                else f"unknown_{max_idx}"
            )

            logger.debug(
                f"ğŸ¯ ì˜ˆì¸¡: {label} ({confidence:.3f}) - {prediction_time:.3f}s"
            )

            return {
                "label": label,
                "confidence": confidence,
                "prediction_time": prediction_time,
                "class_index": int(max_idx),
            }

        except Exception as e:
            logger.error(f"âŒ ìŠ¤ë ˆë“œ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return {"label": "ìŠ¤ë ˆë“œ_ì—ëŸ¬", "confidence": 0.0, "error": str(e)}

    async def predict_multiple(self, keypoints_list: list) -> list:
        """ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•œ ë™ì‹œ ì˜ˆì¸¡"""
        if not keypoints_list:
            return []

        # ëª¨ë“  ì˜ˆì¸¡ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        tasks = [self.predict(keypoints) for keypoints in keypoints_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"ë°°ì¹˜ ì˜ˆì¸¡ {i} ì‹¤íŒ¨: {result}")
                processed_results.append(
                    {"label": "ë°°ì¹˜_ì—ëŸ¬", "confidence": 0.0, "error": str(result)}
                )
            else:
                processed_results.append(result)

        return processed_results

    def is_ready(self) -> bool:
        """ëª¨ë¸ ì¤€ë¹„ ìƒíƒœ"""
        return self.is_loaded and self.model is not None

    def get_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´"""
        return {
            "loaded": self.is_loaded,
            "model_path": self.model_path,
            "num_classes": len(self.class_labels),
            "class_labels": self.class_labels[:10],  # ì²˜ìŒ 10ê°œë§Œ
            "model_type": type(self.model).__name__ if self.model else None,
        }

    async def reload(self, model_path: str = None):
        """ëª¨ë¸ ì¬ë¡œë“œ"""
        logger.info("ğŸ”„ ëª¨ë¸ ì¬ë¡œë“œ...")
        await self.cleanup()
        return await self.initialize(model_path)

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            if self.executor:
                self.executor.shutdown(wait=True)
                self.executor = None

            # ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
            if self.model:
                del self.model
                self.model = None

            # TensorFlow ì„¸ì…˜ ì •ë¦¬
            try:
                import gc

                gc.collect()
                if tf.config.list_physical_devices("GPU"):
                    tf.keras.backend.clear_session()
            except Exception:
                pass

            self.is_loaded = False
            self.class_labels = []
            self.model_path = None

            logger.info("ğŸ§¹ ì •ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
model_manager = ModelManager()


# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ìë™ ì´ˆê¸°í™” (ì„ íƒì )
async def initialize_model_on_startup():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ëª¨ë¸ ì´ˆê¸°í™”"""
    try:
        success = await model_manager.initialize()
        if success:
            logger.info("ğŸš€ ëª¨ë¸ ë§¤ë‹ˆì € ì‹œì‘ ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - ëŸ°íƒ€ì„ì— ì¬ì‹œë„")
    except Exception as e:
        logger.error(f"âŒ ì‹œì‘ ì‹œ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


# FastAPI ì´ë²¤íŠ¸ì—ì„œ ì‚¬ìš©í•  ì´ˆê¸°í™” í•¨ìˆ˜
async def startup_event():
    """FastAPI startup ì´ë²¤íŠ¸"""
    await initialize_model_on_startup()


async def shutdown_event():
    """FastAPI shutdown ì´ë²¤íŠ¸"""
    await model_manager.cleanup()
