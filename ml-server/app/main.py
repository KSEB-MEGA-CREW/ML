import os
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.models.model_loader import ModelManager
from app.api.endpoints import router
from app.utils.logger import setup_logger
from app.config import settings

# logging setting
setup_logger()
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # starting...
    logger.info("ğŸš€ AI ì„œë²„ ì‹œì‘")
    logger.info(f"í™˜ê²½: {settings.DEBUG and 'Development' or 'Production'}")
    logger.info(f"TensorFlow GPU ì‚¬ìš©: {settings.TF_ENABLE_GPU_MEMORY_GROWTH}")

    try:
        # ëª¨ë¸ ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model_manager = ModelManager()

        # ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
        success = await model_manager.load_model()

        if success:
            logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            model_info = model_manager.get_model_info()
            logger.info(f"ëª¨ë¸ ì •ë³´: {model_info}")
        else:
            logger.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            # ê°œë°œ í™˜ê²½ì—ì„œëŠ” ê³„ì† ì§„í–‰, í”„ë¡œë•ì…˜ì—ì„œëŠ” ì¢…ë£Œ
            if not settings.DEBUG:
                raise RuntimeError("ëª¨ë¸ ë¡œë“œ í•„ìˆ˜")

    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
        if not settings.DEBUG:
            raise

    yield

    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ›‘ AI ì„œë²„ ì¢…ë£Œ")
    try:
        model_manager = ModelManager()
        if model_manager.is_ready():
            model_manager.unload_model()
            logger.info("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Sign Language AI Server",
    description="ìˆ˜í™” ì¸ì‹ AI ì„œë²„ - TensorFlow ê¸°ë°˜",
    version="1.0.0",
    lifespan=lifespan,
    debug=settings.DEBUG,
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
if settings.DEBUG:

    @app.middleware("http")
    async def log_requests(request, call_next):
        import time

        start_time = time.time()

        response = await call_next(request)

        process_time = time.time() - start_time
        logger.info(
            f"{request.method} {request.url.path} - "
            f"Status: {response.status_code} - "
            f"Time: {process_time:.3f}s"
        )

        return response


# API ë¼ìš°í„° ë“±ë¡
app.include_router(router, prefix="/api/v1", tags=["AI Analysis"])


# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/", tags=["Root"])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    model_manager = ModelManager()

    return {
        "message": "Sign Language AI Server is running",
        "version": "1.0.0",
        "status": "healthy",
        "model_status": "loaded" if model_manager.is_ready() else "not_loaded",
        "framework": "TensorFlow",
        "endpoints": {
            "health": "/api/v1/health",
            "analyze": "/api/v1/analyze-frame",
            "labels": "/api/v1/labels",
            "model_info": "/api/v1/model-info",
            "docs": "/docs",
        },
    }


# ê°œë°œ ì„œë²„ ì‹¤í–‰ìš© (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
if __name__ == "__main__":
    import uvicorn

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
    host = settings.HOST
    port = settings.PORT
    reload = settings.DEBUG

    logger.info(f"ê°œë°œ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    logger.info(f"API ë¬¸ì„œ: http://{host}:{port}/docs")

    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=reload,
        log_level=settings.LOG_LEVEL.lower(),
    )
